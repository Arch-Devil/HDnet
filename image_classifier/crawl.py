import requests
import pandas as pd
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED
import os

import re


# è¯„è®ºåˆ—è¡¨
comments = []

# æå–è¯„è®ºï¼Œä¼ å…¥å‚æ•°ä¸ºç½‘å€url
def get_comment(url):

    global comments

    try:
        # å‘é€HTTPè¯·æ±‚
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \
                             (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}
        r = requests.get(url=url, headers=headers)

        # è§£æç½‘é¡µï¼Œå®šä½åˆ°è¯„è®ºéƒ¨åˆ†
        soup = BeautifulSoup(r.text, 'lxml')
        main_content = soup.find_all('div', class_='comment_single')

        # æå–è¯„è®º
        for para in main_content:
            comment = para.find('span', class_='heightbox')
            print(comment.text)
            ttxt = comment.text.replace('&quot', '').split('http')[0].split('ï¼Œè¯·æˆ³')[0]
            if len(ttxt)>20:
                comments.append(ttxt)

    except Exception as err:
        print(err)

def main():
    # è¯·æ±‚ç½‘å€
    # urls = ["http://you.ctrip.com/sight/chongqing158/50223-dianping-p%d.html"%x for x in range(1,11)]
    urls = ["http://you.ctrip.com/sight/hangzhou14/49894-dianping-p%d.html"%x for x in range(1,150)]
    urls[0] = urls[0].replace('-p1', '')


    # åˆ©ç”¨å¤šçº¿ç¨‹çˆ¬å–æ™¯ç‚¹è¯„è®º
    executor = ThreadPoolExecutor(max_workers=10)  # å¯ä»¥è‡ªå·±è°ƒæ•´max_workers,å³çº¿ç¨‹çš„ä¸ªæ•°
    # submit()çš„å‚æ•°ï¼š ç¬¬ä¸€ä¸ªä¸ºå‡½æ•°ï¼Œ ä¹‹åä¸ºè¯¥å‡½æ•°çš„ä¼ å…¥å‚æ•°ï¼Œå…è®¸æœ‰å¤šä¸ª
    future_tasks = [executor.submit(get_comment, url) for url in urls]
    # ç­‰å¾…æ‰€æœ‰çš„çº¿ç¨‹å®Œæˆï¼Œæ‰è¿›å…¥åç»­çš„æ‰§è¡Œ
    wait(future_tasks, return_when=ALL_COMPLETED)

    # åˆ›å»ºDataFrameå¹¶ä¿å­˜åˆ°csvæ–‡ä»¶
    comments_table = pd.DataFrame({'id': range(1, len(comments)+1),
                                   'comments': comments})
    print(comments_table)



    data_dir = "./data/comment"
    if not os.path.exists(data_dir):
        os.mkdir(data_dir)

    csv_dir = data_dir+"/hangzhou.csv"
    # dataf = open(data_dir, 'w', newline='')
    # dataf.close()
    comments_table.to_csv(csv_dir, encoding='utf-8-sig', index=False)
    # è¯»å–csvæ–‡ä»¶
    df = pd.read_csv(csv_dir)['comments']
    # å°†pandasä¸­çš„è¯„è®ºä¿®æ”¹åï¼Œå†™å…¥txtæ–‡ä»¶
    for item in df:
        cmt = item.replace('\n', '').replace('&quot', '').replace(r'&gt;', '').replace(r'ï¼ï¼ï¼', 'ï¼') \
            .replace(r' ', '').replace(r'#', '').replace(r'&', '').replace(r'...', '')  \
            .replace(r'ï¼œ', 'ã€Š').replace(r'ï¼', 'ã€‹').replace(r'ï¼ï¼', 'ï¼')  \
            .replace(r'â†‘', '').replace(r'[', '').replace(r']', '') \
            .replace(r'â¤', '').replace(r'â„', '').replace(r'ğŸŒ›', '') \
            .replace(r'âœˆ', '').replace(r',,Ô¾^Ô¾,,', ',') \
            .replace(r'(â—176;u176;â—)â€‹', '').replace(r'ã€', ',') \
            .replace(r'ã€‚ã€‚ã€‚', 'ã€‚').replace(r'ï¼Œï¼Œ', 'ï¼Œ') \
            .replace(r'ğŸš£', '').replace(r',', 'ï¼Œ').replace(r'ï¼Œã€‚', '') \
            .replace(r'ğŸŒ§ï¸', '').replace(r'ğŸŒ', '').replace(r'ã€‚ã€‚', 'ã€‚').replace(r'ï¼ï¼ï¼', 'ï¼')  \
            .replace(r'^_^', '').replace(r'â˜€', '').replace(r'ã€€ã€€', '').replace(r'ğŸŒŸ', '').replace(r'ğŸˆ·ï¸', '')

        cmt = re.sub('[a-zA-Z]', '', cmt)

        with open(data_dir + "/hangzhou.txt", 'a', encoding='utf-8') as f:
            f.write(cmt)
            f.write('\n')
        # print(comments)
def remove_repeat_char(str1):
    list1 = []
    # print(len(str1))
    max_cnt = 0
    max_char = ''
    if len(str1)>0:
        # å…ˆæŠŠç¬¬ä¸€ä¸ªå­—ç¬¦åŠ åˆ°åˆ—è¡¨ä¸­
        list1.append(str1[0])
        cnt = 1
        index = 0
        for i in range(1, len(str1)):
            if str1[i] != list1[index]:
                list1.append(str1[i])
                index += 1
                cnt = 1
            else:
                cnt += 1
                max_cnt = max(cnt,max_cnt)
                max_char = str1[i]
    return max_cnt, max_char

def read_dataset():
    data_dir = "./data/comment"
    # è¯»å–txtæ–‡ä»¶
    filename = data_dir + "/hangzhou.txt"
    with open(filename, 'r', encoding='utf-8') as f:
        raw_text = f.read().lower()


    # åˆ›å»ºä¸€ä¸ªåŒ…å«æ•´ä¸ªæ–‡æœ¬çš„å­—ç¬¦ä¸²
    text_string = ''
    for line in raw_text:
        text_string += line
        # text_string += line.strip()

    # ã€‚åˆ›å»ºä¸€ä¸ªå­—ç¬¦æ•°ç»„
    text = list()
    for char in text_string:
        text.append(char)
    cnt = {}
    for i in text:
        cnt[i] = cnt.get(i,0) + 1
    sort_cnt = sorted(cnt.items(), key=lambda x: x[1], reverse=False)

    print(sort_cnt)
    text_split = text_string.split('\n')
    print(text_split)
    print(len(text_split))
    tstring = ''
    for txt in text_split:
        flag = True
        max_cnt, max_char = remove_repeat_char(txt)
        if max_cnt>2 and 'å“ˆ' not in max_char :
            flag = False
            continue
        print(txt)
        for i in cnt:
            if flag and cnt[i]<15 and txt.find(i)>-1:
                print(i)
                flag = False
        if flag:
            tstring += txt.strip()
    text = list()
    for char in tstring:
        text.append(char)
    # åˆ›å»ºæ–‡å­—å’Œå¯¹åº”æ•°å­—çš„å­—å…¸
    chars = sorted(list(set(text)))
    # å¯¹åŠ è½½æ•°æ®åšæ€»ç»“
    n_chars = len(text)
    n_vocab = len(chars)
    print("æ€»çš„æ–‡å­—æ•°: ", n_chars)
    print("æ€»çš„æ–‡å­—ç±»åˆ«: ", n_vocab)
    with open(data_dir + "/txthangzhou.txt", 'a', encoding='utf-8') as f:
        f.write(tstring)
        f.write('\n')
    return text, n_vocab

main()
read_dataset()



